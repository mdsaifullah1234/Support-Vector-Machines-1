{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS =  The mathematical formula for a linear SVM is:\n",
    "ùëì(X) = W * X +B\n",
    "\n",
    "where \n",
    "w\n",
    "w is the weight vector, \n",
    "x\n",
    "x is the input feature vector, and \n",
    "ùëè\n",
    "b is the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  = The objective function of a linear SVM is to maximize the margin between the support vectors of different classes while minimizing the classification error. It can be formulated as:\n",
    "minimize\n",
    "\n",
    "b is the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = The kernel trick in SVM allows it to implicitly map input vectors into higher-dimensional feature spaces without explicitly computing the transformation. This is done by replacing the dot product between input vectors with a kernel function, such as the radial basis function (RBF) kernel or polynomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Support vectors in SVM are the data points that lie closest to the decision boundary (hyperplane). They determine the position and orientation of the hyperplane and play a crucial role in defining the margin. Only support vectors contribute to the decision boundary and are used to make predictions. For example, in a binary classification problem, the support vectors are the points closest to the decision boundary or lie within the margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Hyperplane: In SVM, the hyperplane separates the data points of different classes in feature space. It is defined by the equation \n",
    "w * X+B = 0\n",
    "\n",
    "Marginal plane: The marginal plane is parallel to the hyperplane and touches the support vectors. It defines the margin of the SVM.\n",
    "Soft margin: Soft margin SVM allows for misclassifications and violations of the margin by introducing slack variables. It is more flexible and tolerant to noisy data.\n",
    "Hard margin: Hard margin SVM does not allow for misclassifications and enforces a strict margin without slack variables. It is less tolerant to noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS =  Implementing SVM through the Iris dataset involves loading the dataset, splitting it into training and testing sets, training a linear SVM classifier on the training set, predicting labels for the testing set, computing the accuracy of the model, and visualizing the decision boundaries.\n",
    "\n",
    "Bonus Task:\n",
    "\n",
    "Load the Iris dataset from scikit-learn.\n",
    "Split the dataset into training and testing sets.\n",
    "Train a linear SVM classifier using scikit-learn.\n",
    "Predict labels for the testing set and compute the accuracy.\n",
    "Plot the decision boundaries of the trained model using two features.\n",
    "Experiment with different values of the regularization parameter \n",
    "ùê∂\n",
    "C to observe its effect on model performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
